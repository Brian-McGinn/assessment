{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "426b2f5e",
   "metadata": {},
   "source": [
    "# Scenario\n",
    "\n",
    "You are given a set of product documentation files (PDFs and Markdown). Your goal is\n",
    "to create a prototype that answers user questions based on these docs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0260540",
   "metadata": {},
   "source": [
    "# Tasks\n",
    "\n",
    "Ingest and chunk the documentation.\n",
    "Store embeddings in a vector database of your choice (e.g., Pinecone, Weaviate,\n",
    "Chroma).\n",
    "Implement a retrieval function.\n",
    "Use an LLM to answer user questions grounded in the retrieved context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25c9075",
   "metadata": {},
   "source": [
    "# Constraints\n",
    "\n",
    "Use LangChain, LlamaIndex, or another modern RAG framework.\n",
    "Include code and a simple architecture diagram showing the flow.\n",
    "Write a short note on how you would scale this for millions of documents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792cc0c9",
   "metadata": {},
   "source": [
    "# Deliverable\n",
    "\n",
    "Code file(s) or notebook link.\n",
    "Architecture diagram (image or PDF).\n",
    "Scaling notes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c61f606",
   "metadata": {},
   "source": [
    "RAG Architecture Diagram\n",
    "\n",
    "![Retrieval Augmented Generation](rag-architecture-diagram.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d2f1f9",
   "metadata": {},
   "source": [
    "RAG pipeline\n",
    "- load file\n",
    "- chunk file\n",
    "- ceate embeddings and store file\n",
    "- retrieve file\n",
    "- Create prompt to inject context\n",
    "- send prompt to llm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79897987",
   "metadata": {},
   "source": [
    "Install libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae74871",
   "metadata": {
    "scrolled": true,
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install langchain\n",
    "!pip install langchain_chroma\n",
    "!pip install langchain_community\n",
    "!pip install langchain-together"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d21c5646",
   "metadata": {},
   "source": [
    "Setup values for later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3d00f0",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "TOGETHER_API_KEY = os.environ.get(\"TOGETHER_API_KEY\", \"Key\")\n",
    "DOC_PATH = \"docs\"\n",
    "CHROMA_PATH = \"chroma_vectors\"\n",
    "EMBEDDING_MODEL = \"BAAI/bge-base-en-v1.5\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c24c4312",
   "metadata": {},
   "source": [
    "Load the documents from a target directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36cb4f77",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader, TextLoader\n",
    "from langchain.schema import Document\n",
    "import os\n",
    "def load_documents_from_directory(directory: str=DOC_PATH) -> list[Document]:\n",
    "    \"\"\"\n",
    "    Load all PDF and Markdown files from a directory into LangChain Document objects.\n",
    "    \"\"\"\n",
    "    documents = []\n",
    "    for filename in os.listdir(directory):\n",
    "        filepath = os.path.join(directory, filename)\n",
    "        \n",
    "        if filename.lower().endswith(\".pdf\"):\n",
    "            loader = PyPDFLoader(filepath)\n",
    "            documents.extend(loader.load())\n",
    "        \n",
    "        elif filename.lower().endswith(\".md\"):\n",
    "            loader = TextLoader(filepath, encoding=\"utf-8\")\n",
    "            documents.extend(loader.load())\n",
    "    \n",
    "    return documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08385e85",
   "metadata": {},
   "source": [
    "Break down the documents into smaller chunks. This will be used for vecor embeddings and reducing the scope of the document context for more accurate information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d48433",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "def split_documents(documents: list[Document]):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=200)\n",
    "    split_text = text_splitter.split_documents(documents)\n",
    "    return split_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a94dff3",
   "metadata": {},
   "source": [
    "Create vector embeddings based on the document chunks. The embeddings will be saved inside a local Chroma database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310d72de",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_together import ChatTogether, TogetherEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "def save_vectors(documents: list[Document]):\n",
    "    embedding = TogetherEmbeddings(model=EMBEDDING_MODEL, api_key=TOGETHER_API_KEY)\n",
    "    Chroma.from_documents(\n",
    "        documents=documents,\n",
    "        embedding=embedding,\n",
    "        persist_directory=CHROMA_PATH,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f37baf",
   "metadata": {},
   "source": [
    "Create a function to retrieve embeddings from the local Chroma DB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43cbb3c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def get_embeddings(query):\n",
    "    if query:\n",
    "        embedding = TogetherEmbeddings(model=EMBEDDING_MODEL, api_key=TOGETHER_API_KEY)\n",
    "        vector_store = Chroma(\n",
    "            persist_directory=CHROMA_PATH,\n",
    "            embedding_function=embedding\n",
    "        )\n",
    "        return vector_store.similarity_search(query, k=3)\n",
    "    return \"No Results Found\"\n",
    "\n",
    "def format_context(docs: list[Document]) -> str:\n",
    "    return \"\\n\\n\".join([doc.page_content for doc in docs])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b78e7fd",
   "metadata": {},
   "source": [
    "Setup the LLM and prompts for your RAG pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f841fb",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain_ollama.chat_models import ChatOllama\n",
    "\n",
    "system_message = (\n",
    "    \"You are a helpful assistant that ONLY answers questions based on the \"\n",
    "    \"provided context. If no relevant context is provided, do NOT answer the \"\n",
    "    \"question and politely inform the user that you don't have the necessary \"\n",
    "    \"information to answer their question accurately.\"\n",
    ")\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    SystemMessagePromptTemplate.from_template(system_message),\n",
    "    HumanMessagePromptTemplate.from_template(\n",
    "        \"Context:\\n{context}\\n\\nQuestion: {query}\"\n",
    "    )\n",
    "])\n",
    "\n",
    "llm = ChatTogether(\n",
    "    model=\"meta-llama/Llama-3.3-70B-Instruct-Turbo-Free\",\n",
    "    api_key=TOGETHER_API_KEY,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400eb254",
   "metadata": {},
   "source": [
    "Now put everything together and see how the query uses the documents to load the proper context for the questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b80a0b7",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    docs = load_documents_from_directory(\"docs\")\n",
    "    doc_chunks = split_documents(docs)\n",
    "    save_vectors(doc_chunks)\n",
    "\n",
    "    query1 = \"How do you install unsloth for LLM fine tuning?\"\n",
    "    context1 = get_embeddings(query1)\n",
    "    messages1 = prompt.format_messages(\n",
    "        context=format_context(context1),\n",
    "        query=query1\n",
    "    )\n",
    "    response1 = llm.invoke(messages1)\n",
    "    print(response1.content)\n",
    "\n",
    "    query2 = \"What is a transformation attention layer?\"\n",
    "    context2 = get_embeddings(query2)\n",
    "    messages2 = prompt.format_messages(\n",
    "        context=format_context(context2),\n",
    "        query=query2\n",
    "    )\n",
    "    response2 = llm.invoke(messages2)\n",
    "    print(response2.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "040e42a6",
   "metadata": {},
   "source": [
    "# Scaling RAG\n",
    "\n",
    "To scale a RAG system to millions of documents, I would focus on a robust, distributed architecture. First, documents would be ingested and normalized in a pipeline that handles PDFs, Markdown, and other formats, with deduplication and semantic chunking for high-quality context. Chunks would be embedded in a distributed vector database that supports sharding, replication, and approximate nearest neighbor search, while a parallel keyword index handles lexical filtering. The retrieval pipeline would combine vector and keyword search, optionally reranking candidates with an LLM or cross-encoder, and always include citations. Caching, batching, and asynchronous processing would optimize throughput, while observability and metrics track recall, latency, and system health. Finally, incremental updates, versioning, and careful resource planning ensure the system can grow efficiently without sacrificing reliability or accuracy.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
