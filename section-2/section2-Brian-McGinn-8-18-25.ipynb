{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c61f606",
   "metadata": {},
   "source": [
    "RAG Architecture Diagram\n",
    "\n",
    "![Retrieval Augmented Generation](rag-architecture-diagram.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d2f1f9",
   "metadata": {},
   "source": [
    "RAG pipeline\n",
    "- load file\n",
    "- chunk file\n",
    "- ceate embeddings and store file\n",
    "- retrieve file\n",
    "- Create prompt to inject context\n",
    "- send prompt to llm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f85a2c",
   "metadata": {},
   "source": [
    "llamaindex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f46bda",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "DOC_PATH=\"docs\"\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, StorageContext, PromptTemplate\n",
    "from llama_index.embeddings.ollama import OllamaEmbedding\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "import chromadb\n",
    "\n",
    "def load_documents_from_directory(directory: str):\n",
    "    \"\"\"\n",
    "    Load PDF and Markdown files from a directory into LlamaIndex Documents.\n",
    "    \"\"\"\n",
    "    return SimpleDirectoryReader(input_dir=directory, required_exts=[\".pdf\", \".md\"]).load_data()\n",
    "\n",
    "def build_rag_system(doc_dir: str, persist_dir: str = \"./chroma_store\"):\n",
    "    # 1. Load documents\n",
    "    documents = load_documents_from_directory(doc_dir)\n",
    "\n",
    "    # 2. Setup embedding model\n",
    "    embed_model = OllamaEmbedding(model_name=\"nomic-embed-text\", request_timeout=120)\n",
    "\n",
    "    # 3. Setup persistent Chroma DB\n",
    "    chroma_client = chromadb.PersistentClient(path=persist_dir)\n",
    "    chroma_collection = chroma_client.get_or_create_collection(\"rag_collection\")\n",
    "    vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "\n",
    "    storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "    # 4. Build index\n",
    "    index = VectorStoreIndex.from_documents(\n",
    "        documents,\n",
    "        storage_context=storage_context,\n",
    "        embed_model=embed_model\n",
    "    )\n",
    "\n",
    "    return index\n",
    "\n",
    "def load_existing_rag(persist_dir: str = \"./chroma_store\"):\n",
    "    \"\"\"\n",
    "    Load an existing persistent Chroma store into LlamaIndex without re-embedding.\n",
    "    \"\"\"\n",
    "    # 1. Setup persistent Chroma client\n",
    "    chroma_client = chromadb.PersistentClient(path=persist_dir)\n",
    "\n",
    "    # 2. Reconnect to the same collection\n",
    "    chroma_collection = chroma_client.get_or_create_collection(\"rag_collection\")\n",
    "\n",
    "    # 3. Wrap in LlamaIndex vector store\n",
    "    vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "    storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "    # 4. Recreate embedding model (Ollama nomic-embed-text)\n",
    "    embed_model = OllamaEmbedding(model_name=\"nomic-embed-text\", request_timeout=120)\n",
    "\n",
    "    # 5. Create index from existing embeddings (no re-embedding)\n",
    "    index = VectorStoreIndex.from_vector_store(vector_store, storage_context=storage_context, embed_model=embed_model)\n",
    "\n",
    "    return index\n",
    "\n",
    "def query_rag(query: str):\n",
    "    index = load_existing_rag()\n",
    "    # Setup Ollama Llama3.1 model\n",
    "    llm = Ollama(model=\"llama3.1\", request_timeout=120)\n",
    "\n",
    "    # Define prompt\n",
    "    qa_template = PromptTemplate(\n",
    "        \"Use the context below to answer the question.\\n\\n\"\n",
    "        \"Context:\\n{context_str}\\n\\n\"\n",
    "        \"Question: {query_str}\\n\\n\"\n",
    "        \"Answer:\"\n",
    "    )\n",
    "\n",
    "    # Create query engine with custom prompt\n",
    "    query_engine = index.as_query_engine(llm=llm, text_qa_template=qa_template)\n",
    "\n",
    "    # Run query\n",
    "    response = query_engine.query(query)\n",
    "    return response\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"\")\n",
    "    build_rag_system(\"./docs\")\n",
    "    user_input = input(\"How can I help?\").lower()\n",
    "    result = query_rag(user_input)\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb4e03c",
   "metadata": {},
   "source": [
    "Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28bac40",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "DOC_PATH=\"docs\"\n",
    "from langchain_community.document_loaders import PyPDFLoader, TextLoader\n",
    "from langchain.schema import Document\n",
    "import os\n",
    "def load_documents_from_directory(directory: str=DOC_PATH):\n",
    "    \"\"\"\n",
    "    Load all PDF and Markdown files from a directory into LangChain Document objects.\n",
    "    \"\"\"\n",
    "    documents = []\n",
    "    for filename in os.listdir(directory):\n",
    "        filepath = os.path.join(directory, filename)\n",
    "        \n",
    "        if filename.lower().endswith(\".pdf\"):\n",
    "            loader = PyPDFLoader(filepath)\n",
    "            documents.extend(loader.load())\n",
    "        \n",
    "        elif filename.lower().endswith(\".md\"):\n",
    "            loader = TextLoader(filepath, encoding=\"utf-8\")\n",
    "            documents.extend(loader.load())\n",
    "    \n",
    "    return documents\n",
    "\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "def save_vectors(documents: list[Document]):\n",
    "    embedding = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "    Chroma.from_documents(\n",
    "        documents=documents,\n",
    "        embedding=embedding,\n",
    "        persist_directory=DOC_PATH,)\n",
    "\n",
    "def create_embeddings():\n",
    "    docs = load_documents_from_directory()\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=200)\n",
    "    split_text = text_splitter.split_documents(docs)\n",
    "    save_vectors(split_text)\n",
    "\n",
    "def get_embeddings(query):\n",
    "    if query:\n",
    "        embedding = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "        vector_store = Chroma(\n",
    "            persist_directory=DOC_PATH,\n",
    "            embedding_function=embedding\n",
    "        )\n",
    "        return vector_store.similarity_search(query, k=3)\n",
    "    return \"No Results Found\"\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "\n",
    "system_message = (\n",
    "    \"You are a helpful assistant that ONLY answers questions based on the \"\n",
    "    \"provided context. If no relevant context is provided, do NOT answer the \"\n",
    "    \"question and politely inform the user that you don't have the necessary \"\n",
    "    \"information to answer their question accurately.\"\n",
    ")\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    SystemMessagePromptTemplate.from_template(system_message),\n",
    "    HumanMessagePromptTemplate.from_template(\n",
    "        \"Context:\\n{context}\\n\\nQuestion: {query}\"\n",
    "    )\n",
    "])\n",
    "\n",
    "from langchain_ollama.chat_models import ChatOllama\n",
    "def call_llm_with_context(query):\n",
    "    llm = ChatOllama(model=\"llama3.1\")\n",
    "    context = get_embeddings(query)\n",
    "    # print(\"Context from embeddings!!!!!!!!!!!!!!!!!!!!\", context)\n",
    "    messages = prompt.format_messages(\n",
    "        context=context,\n",
    "        query=query\n",
    "    )\n",
    "\n",
    "    response = llm.invoke(messages)\n",
    "    print(response.content)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"\")\n",
    "    create_embeddings()\n",
    "    user_input = input(\"How can I help?\").lower()\n",
    "    call_llm_with_context(user_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "040e42a6",
   "metadata": {},
   "source": [
    "Scaling RAG"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
